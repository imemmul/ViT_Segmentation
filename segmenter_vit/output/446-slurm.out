Fri Mar 10 03:10:10 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA TITAN RTX    On   | 00000000:68:00.0 Off |                  N/A |
| 41%   45C    P8    37W / 280W |     15MiB / 24576MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1662      G   /usr/lib/xorg/Xorg                  9MiB |
|    0   N/A  N/A      1814      G   /usr/bin/gnome-shell                4MiB |
+-----------------------------------------------------------------------------+
/cta/users/emir/dev/ViT_Segmentation/segmenter_vit

======================================================================================
SHELL=/bin/bash
SLURM_JOB_USER=emir
SLURM_TASKS_PER_NODE=4
SLURM_JOB_UID=1001007
SLURM_TASK_PID=1797039
CONDA_EXE=/cta/users/emir/anaconda3/bin/conda
_CE_M=
LMOD_arch=x86_64
SLURM_JOB_GPUS=1
SLURM_LOCALID=0
SLURM_SUBMIT_DIR=/cta/users/emir/dev/ViT_Segmentation/segmenter_vit
HOSTNAME=nova32
SLURMD_NODENAME=nova32
SLURM_NODE_ALIASES=(null)
SLURM_CLUSTER_NAME=ozer
SLURM_CPUS_ON_NODE=4
SLURM_JOB_CPUS_PER_NODE=4
LMOD_DIR=/usr/share/lmod/lmod/libexec/
SLURM_GPUS_ON_NODE=1
PWD=/cta/users/emir/dev/ViT_Segmentation/segmenter_vit
SLURM_GTIDS=0
LOGNAME=emir
XDG_SESSION_TYPE=tty
CONDA_PREFIX=/cta/users/emir/anaconda3/envs/emirdlp
SLURM_JOB_PARTITION=main
MODULESHOME=/usr/share/lmod/lmod
MANPATH=/usr/share/lmod/lmod/share/man::
ROCR_VISIBLE_DEVICES=0
LMOD_PREPEND_BLOCK=normal
SLURM_JOB_NUM_NODES=1
SLURM_JOBID=446
SLURM_JOB_QOS=normal
MOTD_SHOWN=pam
HOME=/cta/users/emir
LANG=C.UTF-8
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
SLURM_PROCID=0
CONDA_PROMPT_MODIFIER=(emirdlp) 
TMPDIR=/tmp
SLURM_NTASKS=4
SLURM_TOPOLOGY_ADDR=nova32
LMOD_VERSION=6.6
SSH_CONNECTION=172.31.0.1 54032 10.100.13.10 22
MODULEPATH_ROOT=/usr/modulefiles
SLURM_TOPOLOGY_ADDR_PATTERN=node
CUDA_VISIBLE_DEVICES=0
SLURM_MEM_PER_CPU=1000
LESSCLOSE=/usr/bin/lesspipe %s %s
XDG_SESSION_CLASS=user
LMOD_PKG=/usr/share/lmod/lmod
SLURM_WORKING_CLUSTER=ozer:172.16.1.9:6817:9472:101
TERM=xterm-256color
_CE_CONDA=
LESSOPEN=| /usr/bin/lesspipe %s
USER=emir
SLURM_NODELIST=nova32
ENVIRONMENT=BATCH
GPU_DEVICE_ORDINAL=0
CONDA_SHLVL=1
SLURM_JOB_ACCOUNT=users
SLURM_PRIO_PROCESS=0
SLURM_NPROCS=4
LMOD_SETTARG_CMD=:
SHLVL=2
SLURM_NNODES=1
BASH_ENV=/usr/share/lmod/lmod/init/bash
LMOD_FULL_SETTARG_SUPPORT=no
LMOD_sys=Linux
XDG_SESSION_ID=8719
SLURM_SUBMIT_HOST=login
CONDA_PYTHON_EXE=/cta/users/emir/anaconda3/bin/python
LC_CTYPE=C.UTF-8
XDG_RUNTIME_DIR=/run/user/1001007
LMOD_COLORIZE=yes
SLURM_JOB_ID=446
SLURM_NODEID=0
SSH_CLIENT=172.31.0.1 54032 22
CONDA_DEFAULT_ENV=emirdlp
XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop
SLURM_CONF=/etc/slurm/slurm.conf
PATH=/cta/users/emir/anaconda3/envs/emirdlp/bin:/cta/users/emir/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
SLURM_JOB_NAME=eddy_segmenter
MODULEPATH=/cta/etc/mod
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1001007/bus
LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod
SSH_TTY=/dev/pts/38
SLURM_JOB_GID=1001000
OLDPWD=/cta/users/emir/dev/ViT_Segmentation
SLURM_JOB_NODELIST=nova32
BASH_FUNC_ml%%=() {  eval $($LMOD_DIR/ml_cmd "$@")
}
BASH_FUNC_module%%=() {  eval `$LMOD_CMD bash "$@"`
}
_=/usr/bin/env
======================================================================================

======================================================================================
Setting stack size to unlimited...
real-time non-blocking time  (microseconds, -R) unlimited
core file size              (blocks, -c) 0
data seg size               (kbytes, -d) unlimited
scheduling priority                 (-e) 0
file size                   (blocks, -f) unlimited
pending signals                     (-i) 513707
max locked memory           (kbytes, -l) unlimited
max memory size             (kbytes, -m) 4096000
open files                          (-n) 1024
pipe size                (512 bytes, -p) 8
POSIX message queues         (bytes, -q) 819200
real-time priority                  (-r) 0
stack size                  (kbytes, -s) unlimited
cpu time                   (seconds, -t) unlimited
max user processes                  (-u) 513707
virtual memory              (kbytes, -v) unlimited
file locks                          (-x) unlimited

Running Example Job...!
===============================================================================
Running Python script...
/cta/users/emir/anaconda3/envs/emirdlp/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
2023-03-10 03:10:17,976 - mmseg - INFO - Loaded 9180 images
2023-03-10 03:10:19,119 - mmseg - INFO - Loaded 1620 images
2023-03-10 03:10:19,120 - mmseg - INFO - Start running, host: emir@nova32, work_dir: /cta/users/emir/dev/ViT_Segmentation/segmenter_vit/output
2023-03-10 03:10:19,120 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-03-10 03:10:19,120 - mmseg - INFO - workflow: [('train', 1)], max: 320000 iters
2023-03-10 03:10:19,120 - mmseg - INFO - Checkpoints will be saved to /cta/users/emir/dev/ViT_Segmentation/segmenter_vit/output by HardDiskBackend.
cuda
EncoderDecoder(
  (backbone): VisionTransformer(
    (patch_embed): PatchEmbed(
      (adap_padding): AdaptivePadding()
      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
      (1): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (2): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (3): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (4): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (5): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (6): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (7): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (8): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (9): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (10): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
      (11): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): DropPath()
        )
      )
    )
    (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (decode_head): SegmenterMaskTransformerHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss(avg_non_ignore=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn): FFN(
          (activate): GELU(approximate='none')
          (layers): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
            )
            (1): Linear(in_features=3072, out_features=768, bias=True)
            (2): Dropout(p=0.0, inplace=False)
          )
          (dropout_layer): Identity()
        )
      )
    )
    (dec_proj): Linear(in_features=768, out_features=768, bias=True)
    (patch_proj): Linear(in_features=768, out_features=768, bias=False)
    (classes_proj): Linear(in_features=768, out_features=768, bias=False)
    (decoder_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (mask_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
0.30.0
2023-03-10 03:11:04,958 - mmseg - INFO - Iter [50/320000]	lr: 3.000e-04, eta: 3 days, 9:07:53, time: 0.913, data_time: 0.034, memory: 13066, decode.loss_ce: 0.6896, decode.acc_seg: 98.5121, loss: 0.6896
2023-03-10 03:11:49,045 - mmseg - INFO - Iter [100/320000]	lr: 2.999e-04, eta: 3 days, 7:44:06, time: 0.882, data_time: 0.007, memory: 13066, decode.loss_ce: 0.6824, decode.acc_seg: 98.4822, loss: 0.6824
2023-03-10 03:12:33,058 - mmseg - INFO - Iter [150/320000]	lr: 2.999e-04, eta: 3 days, 7:13:02, time: 0.880, data_time: 0.007, memory: 13066, decode.loss_ce: 0.6753, decode.acc_seg: 98.3877, loss: 0.6753
2023-03-10 03:13:17,902 - mmseg - INFO - Iter [200/320000]	lr: 2.999e-04, eta: 3 days, 7:19:18, time: 0.897, data_time: 0.006, memory: 13066, decode.loss_ce: 0.6682, decode.acc_seg: 98.5669, loss: 0.6682
2023-03-10 03:14:02,730 - mmseg - INFO - Iter [250/320000]	lr: 2.999e-04, eta: 3 days, 7:22:25, time: 0.897, data_time: 0.006, memory: 13066, decode.loss_ce: 0.6613, decode.acc_seg: 98.4349, loss: 0.6613

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHAT IS ATM ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emir/miniconda3/envs/mlptorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/emir/miniconda3/envs/mlptorch/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from mmseg.apis import init_segmentor, inference_segmentor, show_result_pyplot\n",
    "from mmseg.core.evaluation import get_palette\n",
    "from mmcv import Config\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below 2 cell is for registering ATMHead and ATMloss to mmseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ATMHead.__init__() missing 2 required positional arguments: 'img_size' and 'in_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/emir/Desktop/dev/myResearch/src/ViT_Segmentation/segViT/seg_ViT.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlgpu/home/emir/Desktop/dev/myResearch/src/ViT_Segmentation/segViT/seg_ViT.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmseg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecode_heads\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39matm_head\u001b[39;00m \u001b[39mimport\u001b[39;00m ATMHead\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlgpu/home/emir/Desktop/dev/myResearch/src/ViT_Segmentation/segViT/seg_ViT.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmseg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlosses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39matm_loss\u001b[39;00m \u001b[39mimport\u001b[39;00m ATMLoss\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmlgpu/home/emir/Desktop/dev/myResearch/src/ViT_Segmentation/segViT/seg_ViT.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m ATMHead()\n",
      "\u001b[0;31mTypeError\u001b[0m: ATMHead.__init__() missing 2 required positional arguments: 'img_size' and 'in_channels'"
     ]
    }
   ],
   "source": [
    "from mmseg.models.decode_heads.atm_head import ATMHead\n",
    "from mmseg.models.losses.atm_loss import ATMLoss\n",
    "ATMHead() # initiliaze and add register to mmseg module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ATMLoss.__init__() missing 2 required positional arguments: 'num_classes' and 'dec_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/emir/Desktop/dev/myResearch/src/ViT_Segmentation/segViT/seg_ViT.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmlgpu/home/emir/Desktop/dev/myResearch/src/ViT_Segmentation/segViT/seg_ViT.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ATMLoss()\n",
      "\u001b[0;31mTypeError\u001b[0m: ATMLoss.__init__() missing 2 required positional arguments: 'num_classes' and 'dec_layers'"
     ]
    }
   ],
   "source": [
    "ATMLoss() # initiliaze and add register to mmseg module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_Vit_L_cfg = \"./configs/SegViT_L_EddyData.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_segmentor(config=seg_Vit_L_cfg, device=device)\n",
    "main_cfg = Config.fromfile(seg_Vit_L_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.apis import train_segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/emir/Desktop/dev/dataset_eddy/data4test/data/\"\n",
    "label_dir = \"/home/emir/Desktop/dev/dataset_eddy/data4test/label/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_parser import EddyDataset, create_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EddyDataset(input_image_dir=data_dir, mask_image_dir=label_dir, split=0.85, train_bool=True)\n",
    "#valid_data = EddyDataset(input_image_dir=data_dir, mask_image_dir=label_dir, split=0.85, train_bool=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 01:24:52,957 - mmseg - INFO - Start running, host: emir@emir-machine, work_dir: /home/emir/Desktop/dev/myResearch/src/ViT_Segmentation/segViT/output_dir\n",
      "2022-12-15 01:24:52,958 - mmseg - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) PolyLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-12-15 01:24:52,958 - mmseg - INFO - workflow: [('train', 1)], max: 160000 iters\n",
      "2022-12-15 01:24:52,959 - mmseg - INFO - Checkpoints will be saved to /home/emir/Desktop/dev/myResearch/src/ViT_Segmentation/segViT/output_dir by HardDiskBackend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_gpus': 1, 'dist': False, 'seed': 42, 'drop_last': True, 'samples_per_gpu': 32, 'workers_per_gpu': 2}\n",
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EncoderDecoder(\n  (backbone): VisionTransformer(\n    (patch_embed): PatchEmbed(\n      (adap_padding): AdaptivePadding()\n      (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (drop_after_pos): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): Identity()\n        )\n      )\n      (1): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (2): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (3): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (4): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (5): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (6): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (7): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (8): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (9): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (10): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (11): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (12): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (13): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (14): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (15): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (16): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (17): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (18): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (19): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (20): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (21): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (22): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (23): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n    )\n  )\n  (decode_head): ATMHead(\n    input_transform=None, ignore_index=255, align_corners=False\n    (loss_decode): ATMLoss(\n      (criterion): SetCriterion()\n    )\n    (dropout): Dropout2d(p=0.1, inplace=False)\n    (input_proj_1): Linear(in_features=1024, out_features=512, bias=True)\n    (proj_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (decoder_1): TPN_Decoder(\n      (layers): ModuleList(\n        (0): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (2): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n    (input_proj_2): Linear(in_features=1024, out_features=512, bias=True)\n    (proj_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (decoder_2): TPN_Decoder(\n      (layers): ModuleList(\n        (0): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (2): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n    (input_proj_3): Linear(in_features=1024, out_features=512, bias=True)\n    (proj_norm_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (decoder_3): TPN_Decoder(\n      (layers): ModuleList(\n        (0): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (2): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n    (q): Embedding(150, 512)\n    (class_embed): Linear(in_features=512, out_features=151, bias=True)\n  )\n  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n) argument after ** must be a mapping, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/emir/Desktop/dev/myResearch/src/ViT_Segmentation/segViT/seg_ViT.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmlgpu/home/emir/Desktop/dev/myResearch/src/ViT_Segmentation/segViT/seg_ViT.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_segmentor(model\u001b[39m=\u001b[39;49mmodel, cfg\u001b[39m=\u001b[39;49mmain_cfg, validate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dataset\u001b[39m=\u001b[39;49mtrain_data)\n",
      "File \u001b[0;32m~/Desktop/dev/myResearch/src/mmsegmentation/mmseg/apis/train.py:196\u001b[0m, in \u001b[0;36mtrain_segmentor\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    194\u001b[0m     runner\u001b[39m.\u001b[39mload_checkpoint(cfg\u001b[39m.\u001b[39mload_from)\n\u001b[1;32m    195\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(data_loaders))\n\u001b[0;32m--> 196\u001b[0m runner\u001b[39m.\u001b[39;49mrun(data_loaders, cfg\u001b[39m.\u001b[39;49mworkflow)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlptorch/lib/python3.10/site-packages/mmcv/runner/iter_based_runner.py:144\u001b[0m, in \u001b[0;36mIterBasedRunner.run\u001b[0;34m(self, data_loaders, workflow, max_iters, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_iters:\n\u001b[1;32m    143\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m             iter_runner(iter_loaders[i], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)  \u001b[39m# wait for some hooks like loggers to finish\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mafter_epoch\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlptorch/lib/python3.10/site-packages/mmcv/runner/iter_based_runner.py:64\u001b[0m, in \u001b[0;36mIterBasedRunner.train\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_batch \u001b[39m=\u001b[39m data_batch\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mbefore_train_iter\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrain_step(data_batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m):\n\u001b[1;32m     66\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmodel.train_step() must return a dict\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlptorch/lib/python3.10/site-packages/mmcv/parallel/data_parallel.py:77\u001b[0m, in \u001b[0;36mMMDataParallel.train_step\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mmodule must have its parameters and buffers \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     73\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mon device \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj\u001b[39m}\u001b[39;00m\u001b[39m (device_ids[0]) but \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfound one of them on device: \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m inputs, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscatter(inputs, kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids)\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule\u001b[39m.\u001b[39;49mtrain_step(\u001b[39m*\u001b[39;49minputs[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/Desktop/dev/myResearch/src/mmsegmentation/mmseg/models/segmentors/base.py:138\u001b[0m, in \u001b[0;36mBaseSegmentor.train_step\u001b[0;34m(self, data_batch, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\u001b[39mself\u001b[39m, data_batch, optimizer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    113\u001b[0m     \u001b[39m\"\"\"The iteration step during training.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[39m    This method defines an iteration step during training, except for the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m            averaging the logs.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata_batch)\n\u001b[1;32m    139\u001b[0m     loss, log_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_losses(losses)\n\u001b[1;32m    141\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    142\u001b[0m         loss\u001b[39m=\u001b[39mloss,\n\u001b[1;32m    143\u001b[0m         log_vars\u001b[39m=\u001b[39mlog_vars,\n\u001b[1;32m    144\u001b[0m         num_samples\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data_batch[\u001b[39m'\u001b[39m\u001b[39mimg_metas\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: EncoderDecoder(\n  (backbone): VisionTransformer(\n    (patch_embed): PatchEmbed(\n      (adap_padding): AdaptivePadding()\n      (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (drop_after_pos): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): Identity()\n        )\n      )\n      (1): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (2): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (3): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (4): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (5): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (6): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (7): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (8): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (9): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (10): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (11): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (12): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (13): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (14): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (15): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (16): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (17): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (18): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (19): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (20): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (21): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (22): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n      (23): TransformerEncoderLayer(\n        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n          )\n          (proj_drop): Dropout(p=0.0, inplace=False)\n          (dropout_layer): Dropout(p=0.0, inplace=False)\n        )\n        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n        (ffn): FFN(\n          (activate): GELU(approximate='none')\n          (layers): Sequential(\n            (0): Sequential(\n              (0): Linear(in_features=1024, out_features=4096, bias=True)\n              (1): GELU(approximate='none')\n              (2): Dropout(p=0.0, inplace=False)\n            )\n            (1): Linear(in_features=4096, out_features=1024, bias=True)\n            (2): Dropout(p=0.0, inplace=False)\n          )\n          (dropout_layer): DropPath()\n        )\n      )\n    )\n  )\n  (decode_head): ATMHead(\n    input_transform=None, ignore_index=255, align_corners=False\n    (loss_decode): ATMLoss(\n      (criterion): SetCriterion()\n    )\n    (dropout): Dropout2d(p=0.1, inplace=False)\n    (input_proj_1): Linear(in_features=1024, out_features=512, bias=True)\n    (proj_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (decoder_1): TPN_Decoder(\n      (layers): ModuleList(\n        (0): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (2): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n    (input_proj_2): Linear(in_features=1024, out_features=512, bias=True)\n    (proj_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (decoder_2): TPN_Decoder(\n      (layers): ModuleList(\n        (0): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (2): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n    (input_proj_3): Linear(in_features=1024, out_features=512, bias=True)\n    (proj_norm_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (decoder_3): TPN_Decoder(\n      (layers): ModuleList(\n        (0): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (2): TPN_DecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n          (multihead_attn): Attention(\n            (q): Linear(in_features=512, out_features=512, bias=True)\n            (k): Linear(in_features=512, out_features=512, bias=True)\n            (v): Linear(in_features=512, out_features=512, bias=True)\n            (attn_drop): Dropout(p=0.1, inplace=False)\n            (proj): Linear(in_features=512, out_features=512, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n    (q): Embedding(150, 512)\n    (class_embed): Linear(in_features=512, out_features=151, bias=True)\n  )\n  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n) argument after ** must be a mapping, not list"
     ]
    }
   ],
   "source": [
    "train_segmentor(model=model, cfg=main_cfg, validate=False, dataset=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('mlptorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6351ae7a8af7abb5b6e92ecd3cb8e4a903e7aa1ff53853f1ec32897af5c10b9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
